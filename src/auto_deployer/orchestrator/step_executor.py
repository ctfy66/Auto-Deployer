"""Step executor: Executes a single deployment step with LLM guidance."""

from __future__ import annotations

import json
import logging
import platform
from typing import Optional, Union, TYPE_CHECKING

from .models import (
    StepContext, StepResult, StepAction, ActionType,
    CommandRecord, StepStatus, DeployContext, StepOutputs, ExecutionSummary
)
from .prompts import build_step_system_prompt, build_step_user_prompt
from ..llm.output_extractor import CommandOutputExtractor

if TYPE_CHECKING:
    from ..ssh import SSHSession
    from ..local import LocalSession
    from ..interaction import UserInteractionHandler
    from ..config import LLMConfig

logger = logging.getLogger(__name__)


class StepExecutor:
    """
    æ­¥éª¤æ‰§è¡Œå™¨
    
    åœ¨å•ä¸ªæ­¥éª¤çš„è¾¹ç•Œå†…æ‰§è¡Œ LLM å†³ç­–å¾ªç¯ï¼Œç›´åˆ°ï¼š
    - æ­¥éª¤å®Œæˆï¼ˆLLM å£°æ˜ step_doneï¼‰
    - æ­¥éª¤å¤±è´¥ï¼ˆLLM å£°æ˜ step_failed æˆ–è¶…è¿‡æœ€å¤§è¿­ä»£ï¼‰
    """
    
    def __init__(
        self,
        llm_config: "LLMConfig",
        session: Union["SSHSession", "LocalSession"],
        interaction_handler: "UserInteractionHandler",
        max_iterations_per_step: int = 10,
        is_windows: bool = False,
    ):
        self.llm_config = llm_config
        self.session = session
        self.interaction_handler = interaction_handler
        self.max_iterations = max_iterations_per_step
        self.is_windows = is_windows
        
        # Initialize LLM provider using factory
        from ..llm.base import create_llm_provider
        self.llm_provider = create_llm_provider(llm_config)
        logger.info("StepExecutor using LLM provider: %s (model: %s)", llm_config.provider, llm_config.model)

        # è¾“å‡ºæå–å™¨
        self.output_extractor = CommandOutputExtractor(
            max_success_lines=30,
            max_error_lines=50
        )
    def execute(
        self,
        step_ctx: StepContext,
        deploy_ctx: DeployContext,
    ) -> StepResult:
        """
        æ‰§è¡Œå•ä¸ªæ­¥éª¤
        
        Args:
            step_ctx: æ­¥éª¤ä¸Šä¸‹æ–‡ï¼ˆåŒ…å«ç›®æ ‡ã€æˆåŠŸæ ‡å‡†ç­‰ï¼‰
            deploy_ctx: å…¨å±€éƒ¨ç½²ä¸Šä¸‹æ–‡
            
        Returns:
            StepResult: æ­¥éª¤æ‰§è¡Œç»“æœ
        """
        step_ctx.status = StepStatus.RUNNING
        step_ctx.max_iterations = self.max_iterations
        
        logger.info(f"   Goal: {step_ctx.goal}")
        logger.info(f"   Success criteria: {step_ctx.success_criteria}")
        
        for iteration in range(1, self.max_iterations + 1):
            step_ctx.iteration = iteration
            logger.debug(f"   Iteration {iteration}/{self.max_iterations}")
            
            # è·å– LLM å†³ç­–
            action = self._get_next_action(step_ctx, deploy_ctx)
            
            if action.action_type == ActionType.EXECUTE:
                # æ‰§è¡Œå‘½ä»¤
                command = action.command or ""
                logger.info(f"   ğŸ”§ [{iteration}] {command}")
                if action.reasoning:
                    logger.info(f"      ğŸ’­ Reason: {action.reasoning}")
                
                record = self._execute_command(command, action.reasoning)
                step_ctx.commands.append(record)
                
                status = "âœ“" if record.success else "âœ—"
                logger.info(f"      {status} Exit code: {record.exit_code}")
                
                if record.stdout and len(record.stdout) < 200:
                    logger.debug(f"      stdout: {record.stdout}")
                if record.stderr and not record.success:
                    logger.warning(f"      stderr: {record.stderr[:200]}")
                
            elif action.action_type == ActionType.STEP_DONE:
                # æ­¥éª¤å®Œæˆ - éªŒè¯å¹¶å¤„ç†ç»“æ„åŒ–äº§å‡º
                logger.info(f"   âœ… Step completed: {action.message}")
                step_ctx.status = StepStatus.SUCCESS
                
                # éªŒè¯å¹¶è§£æç»“æ„åŒ–äº§å‡º
                structured_outputs = self._validate_outputs(action.outputs)
                if structured_outputs:
                    step_ctx.structured_outputs = structured_outputs
                    step_ctx.outputs = structured_outputs.to_dict()
                    logger.info(f"   ğŸ“¦ Outputs: {structured_outputs.summary}")
                else:
                    # å¦‚æœæ²¡æœ‰æœ‰æ•ˆçš„ç»“æ„åŒ–äº§å‡ºï¼Œåˆ›å»ºä¸€ä¸ªåŸºæœ¬çš„
                    step_ctx.outputs = action.outputs or {}
                    logger.warning("   âš ï¸ No structured outputs provided")
                
                return StepResult.succeeded(
                    outputs=step_ctx.outputs,
                    structured_outputs=structured_outputs
                )
                
            elif action.action_type == ActionType.STEP_FAILED:
                # æ­¥éª¤å¤±è´¥
                logger.error(f"   âŒ Step failed: {action.message}")
                step_ctx.status = StepStatus.FAILED
                step_ctx.error = action.message
                return StepResult.failed(error=action.message or "Unknown error")
                
            elif action.action_type == ActionType.ASK_USER:
                # è¯¢é—®ç”¨æˆ·
                logger.info(f"   ğŸ’¬ Asking user: {action.question}")
                response = self._ask_user(action)
                
                if response.get("cancelled"):
                    logger.info("   User cancelled")
                    step_ctx.status = StepStatus.FAILED
                    return StepResult.failed(error="User cancelled")
                
                step_ctx.user_interactions.append({
                    "question": action.question,
                    "response": response.get("value"),
                })
                logger.info(f"   User replied: {response.get('value')}")
        
        # è¶…è¿‡æœ€å¤§è¿­ä»£
        error_msg = f"Exceeded max iterations ({self.max_iterations}) for this step"
        logger.error(f"   âŒ {error_msg}")
        step_ctx.status = StepStatus.FAILED
        step_ctx.error = error_msg
        return StepResult.failed(error=error_msg)
    
    def _get_next_action(
        self,
        step_ctx: StepContext,
        deploy_ctx: DeployContext,
    ) -> StepAction:
        """è°ƒç”¨ LLM è·å–ä¸‹ä¸€æ­¥åŠ¨ä½œ"""
        
        # ä½¿ç”¨å‡½æ•°å¼ prompt æ„å»ºå™¨ï¼ˆè€Œä¸æ˜¯å·²å¼ƒç”¨çš„å¸¸é‡ï¼‰
        from ..prompts.execution_step import (
            build_step_execution_prompt,
            build_step_execution_prompt_windows
        )
        
        # æ„å»º prompt
        if self.is_windows:
            prompt = build_step_execution_prompt_windows(
                step_id=step_ctx.step_id,
                step_name=step_ctx.step_name,
                category=step_ctx.category,
                goal=step_ctx.goal,
                success_criteria=step_ctx.success_criteria,
                repo_url=deploy_ctx.repo_url,
                deploy_dir=deploy_ctx.deploy_dir,
                host_info=json.dumps(deploy_ctx.host_info, indent=2, ensure_ascii=False),
                commands_history=self._format_commands(step_ctx.commands),
                user_interactions=self._format_interactions(step_ctx.user_interactions),
                max_iterations=self.max_iterations,
                current_iteration=step_ctx.iteration,
            )
        else:
            prompt = build_step_execution_prompt(
                step_id=step_ctx.step_id,
                step_name=step_ctx.step_name,
                category=step_ctx.category,
                goal=step_ctx.goal,
                success_criteria=step_ctx.success_criteria,
                repo_url=deploy_ctx.repo_url,
                deploy_dir=deploy_ctx.deploy_dir,
                host_info=json.dumps(deploy_ctx.host_info, indent=2, ensure_ascii=False),
                commands_history=self._format_commands(step_ctx.commands),
                user_interactions=self._format_interactions(step_ctx.user_interactions),
                max_iterations=self.max_iterations,
                current_iteration=step_ctx.iteration,
                os_type="linux",
            )
        
        # è°ƒç”¨ LLM
        response_text = self._call_llm(prompt)
        
        # è§£æå“åº”
        return self._parse_action(response_text)
    
    def _call_llm(self, prompt: str) -> str:
        """è°ƒç”¨ LLM APIï¼ˆé€šè¿‡ provider æŠ½è±¡å±‚ï¼‰"""
        try:
            response_text = self.llm_provider.generate_response(
                prompt=prompt,
                response_format="json",
                timeout=60,
                max_retries=3
            )
            
            if not response_text:
                logger.error("No response from LLM provider")
                return '{"action": "step_failed", "message": "No LLM response"}'
            
            return response_text
            
        except Exception as e:
            logger.error(f"LLM provider call failed: {e}")
            return f'{{"action": "step_failed", "message": "LLM error: {str(e)}"}}'
    
    def _parse_action(self, text: str) -> StepAction:
        """è§£æ LLM å“åº”ä¸º StepAction"""
        try:
            data = json.loads(text)
            action_str = data.get("action", "step_failed")
            action_map = {
                "execute": ActionType.EXECUTE,
                "step_done": ActionType.STEP_DONE,
                "step_failed": ActionType.STEP_FAILED,
                "ask_user": ActionType.ASK_USER,
            }
            return StepAction(
                action_type=action_map.get(action_str, ActionType.STEP_FAILED),
                command=data.get("command"),
                reasoning=data.get("reasoning"),
                message=data.get("message"),
                question=data.get("question"),
                options=data.get("options"),
                outputs=data.get("outputs"),
            )
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse LLM response: {e}")
            logger.debug(f"Raw response: {text[:200]}")
            return StepAction(
                action_type=ActionType.STEP_FAILED,
                message=f"Failed to parse LLM response: {text[:100]}"
            )
    
    def _execute_command(self, command: str, reasoning: Optional[str] = None) -> CommandRecord:
        """æ‰§è¡Œå‘½ä»¤å¹¶æ™ºèƒ½æå–è¾“å‡º"""
        try:
            result = self.session.run(command, timeout=600, idle_timeout=60)

            # ä½¿ç”¨æ™ºèƒ½æå–å™¨å¤„ç†è¾“å‡º
            extracted = self.output_extractor.extract(
                stdout=result.stdout or "",
                stderr=result.stderr or "",
                success=result.ok,
                exit_code=result.exit_status,
                command=command
            )

            # æ ¼å¼åŒ–ä¸ºLLMå¯è¯»çš„è¾“å‡º
            formatted_output = self.output_extractor.format_for_llm(extracted)

            # æ‰“å°åˆ°ç»ˆç«¯ - æ˜¾ç¤ºæå–åçš„è¾“å‡º
            print("\n" + "=" * 60)
            print("ğŸ“¤ LLMå°†çœ‹åˆ°çš„æå–åè¾“å‡º:")
            print("-" * 60)
            print(formatted_output)
            print("=" * 60 + "\n")

            # è®°å½•åˆ°æ—¥å¿—
            logger.info(f"Extracted output for LLM (original: {extracted.full_length} chars, extracted: {extracted.extracted_length} chars):")
            logger.info(f"Summary: {extracted.summary}")
            if extracted.key_info:
                logger.debug(f"Key info: {extracted.key_info[:5]}")  # åªè®°å½•å‰5æ¡

            # è¿”å›åŒ…å«æå–åè¾“å‡ºå’Œreasoningçš„CommandRecord
            # æ³¨æ„ï¼šCommandRecordéœ€è¦æ‰©å±•ä»¥æ”¯æŒreasoningå’Œextracted_outputå­—æ®µ
            record = CommandRecord(
                command=command,
                success=result.ok,
                exit_code=result.exit_status,
                # ä½¿ç”¨æå–åçš„è¾“å‡ºæ›¿ä»£åŸå§‹æˆªæ–­è¾“å‡º
                stdout=formatted_output,
                stderr="",  # é”™è¯¯å·²æ•´åˆåˆ°stdoutçš„æ ¼å¼åŒ–è¾“å‡ºä¸­
                timestamp=extracted.summary if hasattr(extracted, 'summary') else ""
            )
            
            # ä¸´æ—¶å­˜å‚¨é¢å¤–ä¿¡æ¯ï¼ˆç”¨äºæ—¥å¿—è®°å½•ï¼‰
            record._reasoning = reasoning  # type: ignore
            record._extracted_output = formatted_output  # type: ignore
            record._original_stdout = result.stdout[:2000] if result.stdout else ""  # type: ignore
            record._original_stderr = result.stderr[:2000] if result.stderr else ""  # type: ignore
            
            return record
        except Exception as e:
            logger.error(f"Command execution error: {e}")
            return CommandRecord(
                command=command,
                success=False,
                exit_code=-1,
                stdout="",
                stderr=str(e),
                timestamp=""
            )
    
    def _ask_user(self, action: StepAction) -> dict:
        """è¯¢é—®ç”¨æˆ·"""
        from ..interaction import InteractionRequest, InputType, QuestionCategory
        
        request = InteractionRequest(
            question=action.question or "",
            options=action.options or [],
            input_type=InputType.CHOICE if action.options else InputType.TEXT,
            category=QuestionCategory.DECISION,
            allow_custom=True,
        )
        response = self.interaction_handler.ask(request)
        return {
            "value": response.value,
            "cancelled": response.cancelled,
        }
    
    def _format_commands(self, commands: list) -> str:
        """æ ¼å¼åŒ–å‘½ä»¤å†å²"""
        if not commands:
            return "(no commands executed yet)"
        
        lines = []
        for i, cmd in enumerate(commands[-5:], 1):  # æœ€è¿‘5æ¡
            status = "SUCCESS" if cmd.success else "FAILED"
            lines.append(f"{i}. [{status}] {cmd.command}")
            if cmd.stdout:
                # æˆªå–è¾“å‡º
                stdout_preview = cmd.stdout[:300].replace('\n', '\n   ')
                lines.append(f"   stdout: {stdout_preview}")
            if cmd.stderr and not cmd.success:
                stderr_preview = cmd.stderr[:200].replace('\n', '\n   ')
                lines.append(f"   stderr: {stderr_preview}")
        return "\n".join(lines)
    
    def _format_interactions(self, interactions: list) -> str:
        """æ ¼å¼åŒ–ç”¨æˆ·äº¤äº’å†å²"""
        if not interactions:
            return "(no user interactions)"
        
        lines = []
        for i, item in enumerate(interactions[-3:], 1):
            lines.append(f"{i}. Q: {item['question']}")
            lines.append(f"   A: {item['response']}")
        return "\n".join(lines)
    
    def _validate_outputs(self, outputs_dict: Optional[dict]) -> Optional[StepOutputs]:
        """éªŒè¯å¹¶è§£ææ­¥éª¤äº§å‡º
        
        Args:
            outputs_dict: LLM è¿”å›çš„ outputs å­—å…¸
            
        Returns:
            StepOutputs å¯¹è±¡ï¼Œå¦‚æœéªŒè¯å¤±è´¥åˆ™è¿”å› None
        """
        if not outputs_dict:
            logger.warning("No outputs provided in step_done action")
            return None
        
        if not isinstance(outputs_dict, dict):
            logger.warning(f"Outputs is not a dict: {type(outputs_dict)}")
            return None
        
        # éªŒè¯å¿…å¡«å­—æ®µ
        summary = outputs_dict.get("summary")
        if not summary or not isinstance(summary, str):
            logger.warning("outputs.summary is required and must be a string")
            # å°è¯•ä»å…¶ä»–å­—æ®µç”Ÿæˆæ‘˜è¦
            if outputs_dict.get("message"):
                summary = str(outputs_dict["message"])
            else:
                summary = "Step completed"
        
        try:
            return StepOutputs(
                summary=summary,
                environment_changes=outputs_dict.get("environment_changes", {}),
                new_configurations=outputs_dict.get("new_configurations", {}),
                artifacts=outputs_dict.get("artifacts", []),
                services_started=outputs_dict.get("services_started", []),
                custom_data=outputs_dict.get("custom_data", {}),
                issues_resolved=outputs_dict.get("issues_resolved", []),
            )
        except Exception as e:
            logger.error(f"Failed to create StepOutputs: {e}")
            return None
    
    def _get_next_action_with_summary(
        self,
        step_ctx: StepContext,
        deploy_ctx: DeployContext,
        execution_summary: Optional[ExecutionSummary] = None,
        last_command_result: Optional[dict] = None,
        user_response: Optional[str] = None,
    ) -> StepAction:
        """ä½¿ç”¨æ–°çš„ prompt æ¨¡æ¿è·å– LLM å†³ç­–ï¼ˆå¸¦æ‰§è¡Œæ‘˜è¦ï¼‰
        
        Args:
            step_ctx: æ­¥éª¤ä¸Šä¸‹æ–‡
            deploy_ctx: éƒ¨ç½²ä¸Šä¸‹æ–‡
            execution_summary: å…¨å±€æ‰§è¡Œæ‘˜è¦
            last_command_result: ä¸Šä¸€æ¡å‘½ä»¤çš„ç»“æœ
            user_response: ç”¨æˆ·å›å¤
            
        Returns:
            StepAction å†³ç­–
        """
        # å¦‚æœæœ‰æ‰§è¡Œæ‘˜è¦ï¼Œä½¿ç”¨æ–°çš„ prompt æ¨¡æ¿
        if execution_summary:
            system_prompt = build_step_system_prompt(
                ctx=step_ctx,
                summary=execution_summary,
                is_windows=self.is_windows,
            )
            user_prompt = build_step_user_prompt(
                ctx=step_ctx,
                last_command_result=last_command_result,
                user_response=user_response,
            )
            
            # ç»„åˆ prompt
            full_prompt = f"{system_prompt}\n\n---\n\n{user_prompt}"
            
            # è°ƒç”¨ LLM
            response_text = self._call_llm(full_prompt)
            return self._parse_action(response_text)
        else:
            # å›é€€åˆ°æ—§æ–¹æ³•
            return self._get_next_action(step_ctx, deploy_ctx)

