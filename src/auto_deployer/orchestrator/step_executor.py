"""Step executor: Executes a single deployment step with LLM guidance."""

from __future__ import annotations

import json
import logging
import platform
from datetime import datetime
from typing import Optional, Union, TYPE_CHECKING, Callable

from .models import (
    StepContext, StepResult, StepAction, ActionType,
    CommandRecord, StepStatus, DeployContext, StepOutputs, ExecutionSummary, LoopDetectionResult
)
from .prompts import build_step_system_prompt, build_step_user_prompt

if TYPE_CHECKING:
    from ..ssh import SSHSession
    from ..local import LocalSession
    from ..interaction import UserInteractionHandler
    from ..config import LLMConfig

logger = logging.getLogger(__name__)


class StepExecutor:
    """
    æ­¥éª¤æ‰§è¡Œå™¨
    
    åœ¨å•ä¸ªæ­¥éª¤çš„è¾¹ç•Œå†…æ‰§è¡Œ LLM å†³ç­–å¾ªç¯ï¼Œç›´åˆ°ï¼š
    - æ­¥éª¤å®Œæˆï¼ˆLLM å£°æ˜ step_doneï¼‰
    - æ­¥éª¤å¤±è´¥ï¼ˆLLM å£°æ˜ step_failed æˆ–è¶…è¿‡æœ€å¤§è¿­ä»£ï¼‰
    """
    
    def __init__(
        self,
        llm_config: "LLMConfig",
        session: Union["SSHSession", "LocalSession"],
        interaction_handler: "UserInteractionHandler",
        max_iterations_per_step: int = 10,
        is_windows: bool = False,
        on_command_executed: Optional[Callable[[], None]] = None,
        loop_detection_enabled: bool = True,
    ):
        self.llm_config = llm_config
        self.session = session
        self.interaction_handler = interaction_handler
        self.max_iterations = max_iterations_per_step
        self.is_windows = is_windows
        self.on_command_executed = on_command_executed
        
        # Initialize LLM provider using factory
        from ..llm.base import create_llm_provider
        self.llm_provider = create_llm_provider(llm_config)
        logger.info("StepExecutor using LLM provider: %s (model: %s)", llm_config.provider, llm_config.model)

        # Initialize token manager and history compressor
        from ..llm.token_manager import TokenManager
        from ..llm.history_compressor import HistoryCompressor
        
        self.token_manager = TokenManager(llm_config.provider, llm_config.model)
        self.history_compressor = HistoryCompressor(self.llm_provider)
        
        # Initialize loop detection components
        from .loop_detector import LoopDetector
        from .loop_intervention import LoopInterventionManager
        
        self.loop_detector = LoopDetector(
            enabled=loop_detection_enabled,
            direct_repeat_threshold=3,
            error_loop_threshold=4,
            command_similarity_threshold=0.85,
            output_similarity_threshold=0.80,
        )
        self.loop_intervention_manager = LoopInterventionManager(
            temperature_boost_levels=[0.3, 0.5, 0.7]
        )
        
    def execute(
        self,
        step_ctx: StepContext,
        deploy_ctx: DeployContext,
    ) -> StepResult:
        """
        æ‰§è¡Œå•ä¸ªæ­¥éª¤
        
        Args:
            step_ctx: æ­¥éª¤ä¸Šä¸‹æ–‡ï¼ˆåŒ…å«ç›®æ ‡ã€æˆåŠŸæ ‡å‡†ç­‰ï¼‰
            deploy_ctx: å…¨å±€éƒ¨ç½²ä¸Šä¸‹æ–‡
            
        Returns:
            StepResult: æ­¥éª¤æ‰§è¡Œç»“æœ
        """
        step_ctx.status = StepStatus.RUNNING
        step_ctx.max_iterations = self.max_iterations
        
        # Reset loop intervention manager for this step
        self.loop_intervention_manager.reset()
        
        logger.info(f"   Goal: {step_ctx.goal}")
        logger.info(f"   Success criteria: {step_ctx.success_criteria}")
        
        for iteration in range(1, self.max_iterations + 1):
            step_ctx.iteration = iteration
            logger.debug(f"   Iteration {iteration}/{self.max_iterations}")
            
            # === Loop Detection ===
            if len(step_ctx.commands) >= 3:
                detection = self.loop_detector.check(step_ctx.commands)
                
                if detection.is_loop:
                    logger.warning(f"   ğŸ”„ Loop detected: {detection.loop_type} (confidence: {detection.confidence:.2%})")
                    for evidence in detection.evidence:
                        logger.warning(f"      â€¢ {evidence}")
                    
                    # Decide intervention
                    intervention = self.loop_intervention_manager.decide_intervention(
                        detection, iteration
                    )
                    
                    logger.info(f"   {intervention['message']}")
                    
                    if intervention['action'] == 'boost_temperature':
                        # Boost temperature
                        self.llm_config.temperature = intervention['temperature']
                        logger.info(f"      Temperature: {self.llm_config.temperature}")
                    
                    elif intervention['action'] == 'inject_reflection':
                        # Inject reflection prompt
                        step_ctx.reflection_prompt = intervention['reflection_text']
                        self.llm_config.temperature = intervention['temperature']
                        logger.info(f"      Reflection injected, temperature: {self.llm_config.temperature}")
                    
                    elif intervention['action'] == 'ask_user':
                        # Ask user for intervention
                        self.llm_config.temperature = intervention['temperature']
                        user_decision = self._handle_loop_intervention(detection, step_ctx)
                        
                        if user_decision == 'abort':
                            step_ctx.status = StepStatus.FAILED
                            step_ctx.error = "User aborted due to severe loop"
                            return StepResult.failed(error="User aborted due to severe loop")
                        elif user_decision == 'skip':
                            step_ctx.status = StepStatus.SKIPPED
                            return StepResult.skipped(reason="User skipped due to loop")
                        # Otherwise continue with user guidance
            
            # è·å– LLM å†³ç­–
            action = self._get_next_action(step_ctx, deploy_ctx)
            
            if action.action_type == ActionType.EXECUTE:
                # æ‰§è¡Œå‘½ä»¤
                command = action.command or ""
                logger.info(f"   ğŸ”§ [{iteration}] {command}")
                if action.reasoning:
                    logger.info(f"      ğŸ’­ Reason: {action.reasoning}")
                
                record = self._execute_command(command, action.reasoning)
                step_ctx.commands.append(record)
                
                # ç«‹å³ä¿å­˜æ—¥å¿—
                if self.on_command_executed:
                    self.on_command_executed()
                
                status = "âœ“" if record.success else "âœ—"
                logger.info(f"      {status} Exit code: {record.exit_code}")
                
                if record.stdout and len(record.stdout) < 200:
                    logger.debug(f"      stdout: {record.stdout}")
                if record.stderr and not record.success:
                    logger.warning(f"      stderr: {record.stderr[:200]}")
                
            elif action.action_type == ActionType.STEP_DONE:
                # æ­¥éª¤å®Œæˆ - éªŒè¯å¹¶å¤„ç†ç»“æ„åŒ–äº§å‡º
                logger.info(f"   âœ… Step completed: {action.message}")
                step_ctx.status = StepStatus.SUCCESS
                
                # éªŒè¯å¹¶è§£æç»“æ„åŒ–äº§å‡º
                structured_outputs = self._validate_outputs(action.outputs)
                if structured_outputs:
                    step_ctx.structured_outputs = structured_outputs
                    step_ctx.outputs = structured_outputs.to_dict()
                    logger.info(f"   ğŸ“¦ Outputs: {structured_outputs.summary}")
                else:
                    # å¦‚æœæ²¡æœ‰æœ‰æ•ˆçš„ç»“æ„åŒ–äº§å‡ºï¼Œåˆ›å»ºä¸€ä¸ªåŸºæœ¬çš„
                    step_ctx.outputs = action.outputs or {}
                    logger.warning("   âš ï¸ No structured outputs provided")
                
                return StepResult.succeeded(
                    outputs=step_ctx.outputs,
                    structured_outputs=structured_outputs
                )
                
            elif action.action_type == ActionType.STEP_FAILED:
                # æ­¥éª¤å¤±è´¥
                logger.error(f"   âŒ Step failed: {action.message}")
                step_ctx.status = StepStatus.FAILED
                step_ctx.error = action.message
                return StepResult.failed(error=action.message or "Unknown error")
                
            elif action.action_type == ActionType.ASK_USER:
                # è¯¢é—®ç”¨æˆ·
                logger.info(f"   ğŸ’¬ Asking user: {action.question}")
                response = self._ask_user(action)
                
                if response.get("cancelled"):
                    logger.info("   User cancelled")
                    step_ctx.status = StepStatus.FAILED
                    return StepResult.failed(error="User cancelled")
                
                step_ctx.user_interactions.append({
                    "question": action.question,
                    "response": response.get("value"),
                })
                logger.info(f"   User replied: {response.get('value')}")
        
        # è¶…è¿‡æœ€å¤§è¿­ä»£
        error_msg = f"Exceeded max iterations ({self.max_iterations}) for this step"
        logger.error(f"   âŒ {error_msg}")
        step_ctx.status = StepStatus.FAILED
        step_ctx.error = error_msg
        return StepResult.failed(error=error_msg)
    
    def _get_next_action(
        self,
        step_ctx: StepContext,
        deploy_ctx: DeployContext,
    ) -> StepAction:
        """è°ƒç”¨ LLM è·å–ä¸‹ä¸€æ­¥åŠ¨ä½œ"""
        
        # ä½¿ç”¨å‡½æ•°å¼ prompt æ„å»ºå™¨ï¼ˆè€Œä¸æ˜¯å·²å¼ƒç”¨çš„å¸¸é‡ï¼‰
        from ..prompts.execution_step import (
            build_step_execution_prompt,
            build_step_execution_prompt_windows
        )
        
        # æ„å»º prompt
        if self.is_windows:
            prompt = build_step_execution_prompt_windows(
                step_id=step_ctx.step_id,
                step_name=step_ctx.step_name,
                category=step_ctx.category,
                goal=step_ctx.goal,
                success_criteria=step_ctx.success_criteria,
                repo_url=deploy_ctx.repo_url,
                deploy_dir=deploy_ctx.deploy_dir,
                host_info=json.dumps(deploy_ctx.host_info, indent=2, ensure_ascii=False),
                commands_history=self._format_commands(step_ctx),
                user_interactions=self._format_interactions(step_ctx.user_interactions),
                max_iterations=self.max_iterations,
                current_iteration=step_ctx.iteration,
                estimated_commands=step_ctx.estimated_commands,
            )
        else:
            prompt = build_step_execution_prompt(
                step_id=step_ctx.step_id,
                step_name=step_ctx.step_name,
                category=step_ctx.category,
                goal=step_ctx.goal,
                success_criteria=step_ctx.success_criteria,
                repo_url=deploy_ctx.repo_url,
                deploy_dir=deploy_ctx.deploy_dir,
                host_info=json.dumps(deploy_ctx.host_info, indent=2, ensure_ascii=False),
                commands_history=self._format_commands(step_ctx),
                user_interactions=self._format_interactions(step_ctx.user_interactions),
                max_iterations=self.max_iterations,
                current_iteration=step_ctx.iteration,
                os_type="linux",
                estimated_commands=step_ctx.estimated_commands,
            )
        
        # æ£€æŸ¥tokenä½¿ç”¨é‡ï¼Œå¦‚æœè¾¾åˆ°é˜ˆå€¼åˆ™è§¦å‘å‹ç¼©
        if self.token_manager.should_compress(prompt, threshold=0.5):
            logger.info(f"   ğŸ”„ Token threshold reached at iteration {step_ctx.iteration}, compressing command history...")
            step_ctx = self._compress_step_history(step_ctx)
            
            # é‡æ–°æ„å»ºprompt
            if self.is_windows:
                prompt = build_step_execution_prompt_windows(
                    step_id=step_ctx.step_id,
                    step_name=step_ctx.step_name,
                    category=step_ctx.category,
                    goal=step_ctx.goal,
                    success_criteria=step_ctx.success_criteria,
                    repo_url=deploy_ctx.repo_url,
                    deploy_dir=deploy_ctx.deploy_dir,
                    host_info=json.dumps(deploy_ctx.host_info, indent=2, ensure_ascii=False),
                    commands_history=self._format_commands(step_ctx),
                    user_interactions=self._format_interactions(step_ctx.user_interactions),
                    max_iterations=self.max_iterations,
                    current_iteration=step_ctx.iteration,
                    estimated_commands=step_ctx.estimated_commands,
                )
            else:
                prompt = build_step_execution_prompt(
                    step_id=step_ctx.step_id,
                    step_name=step_ctx.step_name,
                    category=step_ctx.category,
                    goal=step_ctx.goal,
                    success_criteria=step_ctx.success_criteria,
                    repo_url=deploy_ctx.repo_url,
                    deploy_dir=deploy_ctx.deploy_dir,
                    host_info=json.dumps(deploy_ctx.host_info, indent=2, ensure_ascii=False),
                    commands_history=self._format_commands(step_ctx),
                    user_interactions=self._format_interactions(step_ctx.user_interactions),
                    max_iterations=self.max_iterations,
                    current_iteration=step_ctx.iteration,
                    os_type="linux",
                    estimated_commands=step_ctx.estimated_commands,
                )
        
        # æ’å…¥åæ€promptï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if step_ctx.reflection_prompt:
            prompt = step_ctx.reflection_prompt + "\n\n" + prompt
            logger.debug("   Reflection prompt injected into LLM call")
            # æ¸…é™¤åæ€promptï¼Œé¿å…é‡å¤æ³¨å…¥
            step_ctx.reflection_prompt = None
        
        # è°ƒç”¨ LLM
        response_text = self._call_llm(prompt)
        
        # è§£æå“åº”
        return self._parse_action(response_text)
    
    def _call_llm(self, prompt: str) -> str:
        """è°ƒç”¨ LLM APIï¼ˆé€šè¿‡ provider æŠ½è±¡å±‚ï¼‰"""
        try:
            response_text = self.llm_provider.generate_response(
                prompt=prompt,
                response_format="json",
                timeout=60,
                max_retries=3
            )
            
            if not response_text:
                logger.error("No response from LLM provider")
                return '{"action": "step_failed", "message": "No LLM response"}'
            
            return response_text
            
        except Exception as e:
            logger.error(f"LLM provider call failed: {e}")
            return f'{{"action": "step_failed", "message": "LLM error: {str(e)}"}}'
    
    def _parse_action(self, text: str) -> StepAction:
        """è§£æ LLM å“åº”ä¸º StepAction"""
        try:
            data = json.loads(text)
            action_str = data.get("action", "step_failed")
            action_map = {
                "execute": ActionType.EXECUTE,
                "step_done": ActionType.STEP_DONE,
                "step_failed": ActionType.STEP_FAILED,
                "ask_user": ActionType.ASK_USER,
            }
            return StepAction(
                action_type=action_map.get(action_str, ActionType.STEP_FAILED),
                command=data.get("command"),
                reasoning=data.get("reasoning"),
                message=data.get("message"),
                question=data.get("question"),
                options=data.get("options"),
                outputs=data.get("outputs"),
            )
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse LLM response: {e}")
            logger.debug(f"Raw response: {text[:200]}")
            return StepAction(
                action_type=ActionType.STEP_FAILED,
                message=f"Failed to parse LLM response: {text[:100]}"
            )
    
    def _get_smart_timeout(self, command: str) -> tuple[int, int]:
        """æ ¹æ®å‘½ä»¤å†…å®¹è¿”å›åˆç†çš„è¶…æ—¶å€¼
        
        Args:
            command: è¦æ‰§è¡Œçš„å‘½ä»¤
            
        Returns:
            (timeout, idle_timeout) å…ƒç»„
        """
        import re
        
        # é»˜è®¤å€¼
        timeout = 600        # 10åˆ†é’Ÿ
        idle_timeout = 60    # 1åˆ†é’Ÿ
        
        # æ£€æµ‹sleep/waitå‘½ä»¤ï¼Œå»¶é•¿æ€»è¶…æ—¶
        sleep_patterns = [
            r'sleep\s+(\d+)',                    # Linux: sleep 300
            r'Start-Sleep\s+-Seconds\s+(\d+)',   # PowerShell: Start-Sleep -Seconds 300
            r'timeout\s+/t\s+(\d+)',             # Windows CMD: timeout /t 300
        ]
        for pattern in sleep_patterns:
            match = re.search(pattern, command, re.IGNORECASE)
            if match:
                sleep_duration = int(match.group(1))
                # æ€»è¶…æ—¶ = sleepæ—¶é—´ + 120ç§’ä½™é‡
                timeout = max(timeout, sleep_duration + 120)
                break
        
        # æ£€æµ‹é•¿æ—¶é—´è¿è¡Œçš„æ„å»º/å®‰è£…å‘½ä»¤
        long_running_commands = [
            'npm install',
            'npm ci',
            'pnpm install',
            'pnpm i',
            'yarn install',
            'pip install',
            'docker build',
            'docker compose up',
            'docker-compose up',
            'cargo build',
            'mvn install',
            'gradle build',
        ]
        
        command_lower = command.lower()
        if any(cmd in command_lower for cmd in long_running_commands):
            timeout = 1800       # 30åˆ†é’Ÿ
            idle_timeout = 180   # 3åˆ†é’Ÿ
        
        # æ£€æµ‹monitoringå‘½ä»¤ï¼ˆå¸¦-fæˆ–--followæ ‡å¿—ï¼‰
        if re.search(r'-f\b|--follow\b', command):
            idle_timeout = 300   # 5åˆ†é’Ÿ
        
        return timeout, idle_timeout
    
    def _execute_command(self, command: str, reasoning: Optional[str] = None) -> CommandRecord:
        """æ‰§è¡Œå‘½ä»¤å¹¶ä¿å­˜å®Œæ•´è¾“å‡º"""
        try:
            # æ™ºèƒ½æ£€æµ‹è¶…æ—¶å‚æ•°
            timeout, idle_timeout = self._get_smart_timeout(command)
            
            # è®°å½•ä½¿ç”¨çš„è¶…æ—¶å€¼ï¼ˆè°ƒè¯•ç”¨ï¼‰
            logger.debug(f"Executing command with timeout={timeout}s, idle_timeout={idle_timeout}s")
            
            result = self.session.run(command, timeout=timeout, idle_timeout=idle_timeout)

            # ç›´æ¥ä½¿ç”¨å®Œæ•´è¾“å‡ºï¼Œä¸å†æå–
            record = CommandRecord(
                command=command,
                success=result.ok,
                exit_code=result.exit_status,
                stdout=result.stdout or "",
                stderr=result.stderr or "",
                timestamp=datetime.now().isoformat()
            )
            
            # æ—¥å¿—è¾“å‡ºï¼ˆå¯é€‰æ‹©æ€§æˆªæ–­æ˜¾ç¤ºï¼‰
            logger.info(f"Command executed: {command}")
            logger.info(f"Exit code: {result.exit_status}")
            
            # åªåœ¨ç»ˆç«¯æ˜¾ç¤ºç®€çŸ­æ‘˜è¦
            if result.stdout and len(result.stdout) < 500:
                logger.debug(f"stdout preview: {result.stdout[:500]}")
            elif result.stdout:
                logger.debug(f"stdout: {len(result.stdout)} characters")
            
            if result.stderr:
                if not result.ok:
                    # å¤±è´¥æ—¶æ˜¾ç¤ºé”™è¯¯
                    logger.warning(f"stderr: {result.stderr[:500]}")
                else:
                    logger.debug(f"stderr: {len(result.stderr)} characters")
            
            return record
            
        except Exception as e:
            logger.error(f"Command execution error: {e}")
            return CommandRecord(
                command=command,
                success=False,
                exit_code=-1,
                stdout="",
                stderr=str(e),
                timestamp=datetime.now().isoformat()
            )
    
    def _ask_user(self, action: StepAction) -> dict:
        """è¯¢é—®ç”¨æˆ·"""
        from ..interaction import InteractionRequest, InputType, QuestionCategory
        
        request = InteractionRequest(
            question=action.question or "",
            options=action.options or [],
            input_type=InputType.CHOICE if action.options else InputType.TEXT,
            category=QuestionCategory.DECISION,
            allow_custom=True,
        )
        response = self.interaction_handler.ask(request)
        return {
            "value": response.value,
            "cancelled": response.cancelled,
        }
    
    def _handle_loop_intervention(self, detection: LoopDetectionResult, step_ctx: StepContext) -> str:
        """å¤„ç†ä¸¥é‡å¾ªç¯ï¼Œè¯¢é—®ç”¨æˆ·å¦‚ä½•å¤„ç†
        
        Args:
            detection: å¾ªç¯æ£€æµ‹ç»“æœ
            step_ctx: æ­¥éª¤ä¸Šä¸‹æ–‡
            
        Returns:
            str: ç”¨æˆ·å†³å®š ("continue" | "skip" | "abort" | "guidance")
        """
        from ..interaction import InteractionRequest, InputType, QuestionCategory
        
        evidence_text = "\n".join(f"  â€¢ {e}" for e in detection.evidence)
        
        question = f"""
Agent appears stuck in a loop ({detection.loop_type}, confidence: {detection.confidence:.1%}):

{evidence_text}

This is the {self.loop_intervention_manager.loop_count}th time a loop has been detected.

What would you like to do?
"""
        
        request = InteractionRequest(
            question=question,
            options=[
                "Continue (let agent try with higher temperature)",
                "Skip this step",
                "Abort deployment",
                "Provide guidance (custom input)"
            ],
            input_type=InputType.CHOICE,
            category=QuestionCategory.ERROR_RECOVERY,
            allow_custom=True,
        )
        
        response = self.interaction_handler.ask(request)
        
        if response.cancelled:
            return "abort"
        
        choice = response.value.lower()
        
        if "continue" in choice:
            return "continue"
        elif "skip" in choice:
            return "skip"
        elif "abort" in choice:
            return "abort"
        elif "guidance" in choice or "custom" in choice:
            # Ask for custom guidance
            guidance_request = InteractionRequest(
                question="Please provide guidance for the agent:",
                options=[],
                input_type=InputType.TEXT,
                category=QuestionCategory.INFORMATION,
            )
            guidance_response = self.interaction_handler.ask(guidance_request)
            
            if not guidance_response.cancelled and guidance_response.value:
                # Inject user guidance as reflection
                step_ctx.reflection_prompt = f"""
USER GUIDANCE:
{guidance_response.value}

Please follow the user's guidance carefully.
"""
                logger.info(f"   User guidance injected: {guidance_response.value[:100]}...")
            
            return "continue"
        else:
            return "continue"
    
    
    def _compress_step_history(self, step_ctx: StepContext) -> StepContext:
        """å‹ç¼©å½“å‰æ­¥éª¤çš„å‘½ä»¤å†å²
        
        ä¿ç•™æœ€è¿‘30%çš„å‘½ä»¤ï¼Œå‹ç¼©è¾ƒè¿œçš„70%
        """
        from datetime import datetime
        from .models import CompressionEvent
        
        total_commands = len(step_ctx.commands)
        if total_commands < 10:
            # å‘½ä»¤å¤ªå°‘ä¸å‹ç¼©
            logger.debug(f"   Skipping compression: only {total_commands} commands")
            return step_ctx
        
        # ä¿ç•™æœ€è¿‘30%ï¼ˆè‡³å°‘ä¿ç•™3æ¡ï¼‰
        keep_count = max(3, int(total_commands * 0.3))
        recent_commands = step_ctx.commands[-keep_count:]
        old_commands = step_ctx.commands[:-keep_count]
        
        logger.debug(f"   Compressing {len(old_commands)} commands, keeping {len(recent_commands)} recent")
        
        # è®¡ç®—å‹ç¼©å‰çš„tokenæ•°é‡
        token_count_before = None
        try:
            # æ„å»ºå®Œæ•´çš„å‘½ä»¤å†å²æ–‡æœ¬ç”¨äºtokenè®¡æ•°
            full_history = self._format_commands(step_ctx)
            token_count_before = self.token_manager.count_tokens(full_history)
        except Exception as e:
            logger.debug(f"   Failed to count tokens before compression: {e}")
        
        # è°ƒç”¨LLMå‹ç¼©
        try:
            compressed_text = self.history_compressor.compress(
                commands=old_commands,
                step_name=step_ctx.step_name,
                step_goal=step_ctx.goal,
            )
            
            # æ›´æ–°ä¸Šä¸‹æ–‡
            step_ctx.compressed_history = compressed_text
            step_ctx.commands = recent_commands
            
            # è®¡ç®—å‹ç¼©åçš„tokenæ•°é‡
            token_count_after = None
            compression_ratio = 0.0
            try:
                new_history = self._format_commands(step_ctx)
                token_count_after = self.token_manager.count_tokens(new_history)
                
                if token_count_before and token_count_after:
                    compression_ratio = ((token_count_before - token_count_after) / token_count_before) * 100
            except Exception as e:
                logger.debug(f"   Failed to count tokens after compression: {e}")
            
            # è·å–tokené™åˆ¶ç”¨äºè§¦å‘åŸå› 
            token_limit = self.token_manager.get_limit()
            trigger_reason = f"Token threshold 50% reached ({token_count_before}/{token_limit} tokens)" if token_count_before else "Token threshold reached"
            
            # åˆ›å»ºå‹ç¼©äº‹ä»¶è®°å½•
            compression_event = CompressionEvent(
                iteration=step_ctx.iteration,
                commands_before=total_commands,
                commands_compressed=len(old_commands),
                commands_kept=len(recent_commands),
                compressed_text_length=len(compressed_text),
                token_count_before=token_count_before,
                token_count_after=token_count_after,
                compression_ratio=compression_ratio,
                timestamp=datetime.now().isoformat(),
                trigger_reason=trigger_reason,
            )
            
            # æ·»åŠ åˆ°å‹ç¼©äº‹ä»¶åˆ—è¡¨
            step_ctx.compression_events.append(compression_event)
            
            # è¾“å‡ºè¯¦ç»†çš„å‹ç¼©æ—¥å¿—
            logger.info(f"   âœ“ History compressed at iteration {step_ctx.iteration}:")
            logger.info(f"      Commands: {total_commands} total â†’ {len(old_commands)} compressed + {len(recent_commands)} kept")
            if token_count_before and token_count_after:
                logger.info(f"      Tokens: {token_count_before} â†’ {token_count_after} ({compression_ratio:.1f}% saved)")
            logger.info(f"      Compressed text: {len(compressed_text)} chars")
            
        except Exception as e:
            logger.error(f"   âœ— Compression failed: {e}, keeping all commands")
        
        return step_ctx
    
    def _format_commands(self, step_ctx: StepContext) -> str:
        """æ ¼å¼åŒ–å‘½ä»¤å†å²ï¼ˆæ”¯æŒå‹ç¼©ï¼‰"""
        lines = []
        
        # å¦‚æœæœ‰å‹ç¼©å†å²ï¼Œå…ˆæ·»åŠ 
        if step_ctx.compressed_history:
            lines.append("=== Earlier Commands (Compressed) ===")
            lines.append(step_ctx.compressed_history)
            lines.append("")
            lines.append("=== Recent Commands (Full Detail) ===")
        
        # æ·»åŠ æœ€è¿‘çš„å®Œæ•´å‘½ä»¤
        if not step_ctx.commands:
            lines.append("(no recent commands)")
        else:
            for i, cmd in enumerate(step_ctx.commands, 1):
                status = "SUCCESS" if cmd.success else f"FAILED"
                lines.append(f"{i}. [{status}] {cmd.command}")
                
                # å®Œæ•´è¾“å‡ºï¼Œä¸å†æˆªæ–­
                if cmd.stdout:
                    lines.append(f"   stdout:")
                    for line in cmd.stdout.split('\n'):
                        lines.append(f"     {line}")
                
                if cmd.stderr:
                    lines.append(f"   stderr:")
                    for line in cmd.stderr.split('\n'):
                        lines.append(f"     {line}")
        
        return "\n".join(lines)
    
    def _format_interactions(self, interactions: list) -> str:
        """æ ¼å¼åŒ–ç”¨æˆ·äº¤äº’å†å²"""
        if not interactions:
            return "(no user interactions)"
        
        lines = []
        for i, item in enumerate(interactions[-3:], 1):
            lines.append(f"{i}. Q: {item['question']}")
            lines.append(f"   A: {item['response']}")
        return "\n".join(lines)
    
    def _validate_outputs(self, outputs_dict: Optional[dict]) -> Optional[StepOutputs]:
        """éªŒè¯å¹¶è§£ææ­¥éª¤äº§å‡ºï¼ˆç®€åŒ–ç‰ˆï¼‰
        
        Args:
            outputs_dict: LLM è¿”å›çš„ outputs å­—å…¸
            
        Returns:
            StepOutputs å¯¹è±¡ï¼Œå¦‚æœéªŒè¯å¤±è´¥åˆ™è¿”å› None
        """
        if not outputs_dict:
            logger.warning("No outputs provided in step_done action")
            return None
        
        if not isinstance(outputs_dict, dict):
            logger.warning(f"Outputs is not a dict: {type(outputs_dict)}")
            return None
        
        # éªŒè¯å¿…å¡«å­—æ®µ
        summary = outputs_dict.get("summary")
        if not summary or not isinstance(summary, str):
            logger.warning("outputs.summary is required and must be a string")
            # å°è¯•ä»å…¶ä»–å­—æ®µç”Ÿæˆæ‘˜è¦
            if outputs_dict.get("message"):
                summary = str(outputs_dict["message"])
            else:
                summary = "Step completed"
        
        # æå– key_infoï¼ˆå¯é€‰ï¼‰
        key_info = outputs_dict.get("key_info", {})
        if not isinstance(key_info, dict):
            logger.warning(f"key_info should be a dict, got {type(key_info)}")
            key_info = {}
        
        try:
            return StepOutputs(
                summary=summary,
                key_info=key_info,
            )
        except Exception as e:
            logger.error(f"Failed to create StepOutputs: {e}")
            return None
    
    def _get_next_action_with_summary(
        self,
        step_ctx: StepContext,
        deploy_ctx: DeployContext,
        execution_summary: Optional[ExecutionSummary] = None,
        last_command_result: Optional[dict] = None,
        user_response: Optional[str] = None,
    ) -> StepAction:
        """ä½¿ç”¨æ–°çš„ prompt æ¨¡æ¿è·å– LLM å†³ç­–ï¼ˆå¸¦æ‰§è¡Œæ‘˜è¦ï¼‰
        
        Args:
            step_ctx: æ­¥éª¤ä¸Šä¸‹æ–‡
            deploy_ctx: éƒ¨ç½²ä¸Šä¸‹æ–‡
            execution_summary: å…¨å±€æ‰§è¡Œæ‘˜è¦
            last_command_result: ä¸Šä¸€æ¡å‘½ä»¤çš„ç»“æœ
            user_response: ç”¨æˆ·å›å¤
            
        Returns:
            StepAction å†³ç­–
        """
        # å¦‚æœæœ‰æ‰§è¡Œæ‘˜è¦ï¼Œä½¿ç”¨æ–°çš„ prompt æ¨¡æ¿
        if execution_summary:
            system_prompt = build_step_system_prompt(
                ctx=step_ctx,
                summary=execution_summary,
                is_windows=self.is_windows,
            )
            user_prompt = build_step_user_prompt(
                ctx=step_ctx,
                last_command_result=last_command_result,
                user_response=user_response,
            )
            
            # ç»„åˆ prompt
            full_prompt = f"{system_prompt}\n\n---\n\n{user_prompt}"
            
            # è°ƒç”¨ LLM
            response_text = self._call_llm(full_prompt)
            return self._parse_action(response_text)
        else:
            # å›é€€åˆ°æ—§æ–¹æ³•
            return self._get_next_action(step_ctx, deploy_ctx)

