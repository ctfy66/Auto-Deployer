"""å¢å¼ºçš„æµ‹è¯•æŠ¥å‘Šç”Ÿæˆå™¨ - ç”ŸæˆåŒ…å«è¯¦ç»†ä¿¡æ¯çš„æµ‹è¯•æŠ¥å‘Š"""
import json
import statistics
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Optional

from .enhanced_metrics import EnhancedProjectMetrics, SystemInfo, LLMConfig


class EnhancedReportGenerator:
    """å¢å¼ºçš„æµ‹è¯•æŠ¥å‘Šç”Ÿæˆå™¨"""
    
    def __init__(self, output_dir: Path = Path("tests/results/reports")):
        """
        åˆå§‹åŒ–æŠ¥å‘Šç”Ÿæˆå™¨
        
        Args:
            output_dir: æŠ¥å‘Šè¾“å‡ºç›®å½•
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
    
    def generate_enhanced_json_report(
        self,
        results: List[EnhancedProjectMetrics],
        test_start_time: datetime,
        test_end_time: datetime,
        parallel_workers: int
    ) -> Path:
        """
        ç”Ÿæˆå¢å¼ºçš„JSONæ ¼å¼æŠ¥å‘Š
        
        Args:
            results: å¢å¼ºçš„é¡¹ç›®æŒ‡æ ‡åˆ—è¡¨
            test_start_time: æµ‹è¯•å¼€å§‹æ—¶é—´
            test_end_time: æµ‹è¯•ç»“æŸæ—¶é—´
            parallel_workers: å¹¶è¡Œworkeræ•°é‡
            
        Returns:
            æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = self.output_dir / f"parallel_test_report_{timestamp}.json"
        
        # æ„å»ºæŠ¥å‘Šæ•°æ®
        report_data = {
            "report_metadata": self._build_report_metadata(
                test_start_time, test_end_time, parallel_workers
            ),
            "test_environment": self._build_environment_section(results),
            "summary": self._aggregate_enhanced_metrics(results),
            "projects": [r.to_dict() for r in results]
        }
        
        with open(report_file, "w", encoding="utf-8") as f:
            json.dump(report_data, f, indent=2, ensure_ascii=False)
        
        return report_file
    
    def generate_enhanced_markdown_report(
        self,
        results: List[EnhancedProjectMetrics],
        summary: Dict[str, Any],
        test_start_time: datetime,
        test_end_time: datetime
    ) -> Path:
        """
        ç”Ÿæˆå¢å¼ºçš„Markdownæ ¼å¼æŠ¥å‘Š
        
        Args:
            results: å¢å¼ºçš„é¡¹ç›®æŒ‡æ ‡åˆ—è¡¨
            summary: èšåˆæ‘˜è¦
            test_start_time: æµ‹è¯•å¼€å§‹æ—¶é—´
            test_end_time: æµ‹è¯•ç»“æŸæ—¶é—´
            
        Returns:
            æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = self.output_dir / f"parallel_test_report_{timestamp}.md"
        
        lines = []
        
        # æ ‡é¢˜
        lines.append("# Auto-Deployer å¹¶è¡Œæµ‹è¯•æŠ¥å‘Š")
        lines.append("")
        lines.append(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        lines.append(f"**æµ‹è¯•æ—¶é•¿**: {(test_end_time - test_start_time).total_seconds() / 60:.1f} åˆ†é’Ÿ")
        lines.append("")
        
        # æµ‹è¯•ç¯å¢ƒ
        if results:
            lines.append("## ğŸ–¥ï¸ æµ‹è¯•ç¯å¢ƒ")
            lines.append("")
            
            first_result = results[0]
            if first_result.system_info:
                sys_info = first_result.system_info
                lines.append(f"- **æ“ä½œç³»ç»Ÿ**: {sys_info.os_name} {sys_info.os_version}")
                lines.append(f"- **Pythonç‰ˆæœ¬**: {sys_info.python_version}")
                lines.append(f"- **ä¸»æœºå**: {sys_info.hostname}")
                lines.append(f"- **CPUæ ¸å¿ƒæ•°**: {sys_info.cpu_count}")
                lines.append(f"- **æ€»å†…å­˜**: {sys_info.memory_total_gb} GB")
                lines.append("")
            
            if first_result.llm_config:
                llm_cfg = first_result.llm_config
                lines.append("### LLM é…ç½®")
                lines.append("")
                lines.append(f"- **æä¾›å•†**: {llm_cfg.provider}")
                lines.append(f"- **æ¨¡å‹**: {llm_cfg.model}")
                lines.append(f"- **æ¸©åº¦**: {llm_cfg.temperature}")
                lines.append(f"- **æœ€å¤§è¿­ä»£æ¬¡æ•°**: {llm_cfg.max_iterations}")
                lines.append(f"- **æ¯æ­¥æœ€å¤§è¿­ä»£**: {llm_cfg.max_iterations_per_step}")
                lines.append(f"- **å¯ç”¨è§„åˆ’**: {'æ˜¯' if llm_cfg.enable_planning else 'å¦'}")
                lines.append("")
        
        # æµ‹è¯•æ‘˜è¦
        lines.append("## ğŸ“Š æµ‹è¯•æ‘˜è¦")
        lines.append("")
        lines.append(f"- **æ€»é¡¹ç›®æ•°**: {summary['total_projects']}")
        lines.append(f"- **æˆåŠŸ**: {summary['successful']} âœ…")
        lines.append(f"- **å¤±è´¥**: {summary['failed']} âŒ")
        lines.append(f"- **æˆåŠŸç‡**: {summary['success_rate']:.1f}%")
        
        if summary.get('total_retries', 0) > 0:
            lines.append(f"- **æ€»é‡è¯•æ¬¡æ•°**: {summary['total_retries']}")
        
        lines.append("")
        
        # æ€§èƒ½ç»Ÿè®¡
        if summary.get('avg_metrics'):
            lines.append("### âš¡ æ€§èƒ½ç»Ÿè®¡ï¼ˆä»…æˆåŠŸé¡¹ç›®ï¼‰")
            lines.append("")
            avg = summary['avg_metrics']
            lines.append(f"- **å¹³å‡éƒ¨ç½²æ—¶é—´**: {avg['deployment_time_seconds']:.1f} ç§’")
            lines.append(f"- **å¹³å‡è¿­ä»£æ¬¡æ•°**: {avg['iterations']:.1f}")
            lines.append(f"- **å¹³å‡å‘½ä»¤æ•°**: {avg['commands']:.1f}")
            lines.append(f"- **å¹³å‡LLMè°ƒç”¨**: {avg['llm_calls']:.1f}")
            lines.append("")
        
        # æŒ‰éš¾åº¦åˆ†ç±»
        if summary.get("by_difficulty"):
            lines.append("### ğŸ“ˆ æŒ‰éš¾åº¦åˆ†ç±»")
            lines.append("")
            lines.append("| éš¾åº¦ | æˆåŠŸ | æ€»æ•° | æˆåŠŸç‡ |")
            lines.append("|------|------|------|--------|")
            for diff in ["easy", "medium", "hard"]:
                if diff in summary["by_difficulty"]:
                    stats = summary["by_difficulty"][diff]
                    lines.append(
                        f"| {diff.capitalize()} | {stats['success']} | "
                        f"{stats['total']} | {stats['success_rate']:.1f}% |"
                    )
            lines.append("")
        
        # æŒ‰ç­–ç•¥åˆ†ç±»
        if summary.get("by_strategy"):
            lines.append("### ğŸ¯ æŒ‰ç­–ç•¥åˆ†ç±»")
            lines.append("")
            lines.append("| ç­–ç•¥ | æˆåŠŸ | æ€»æ•° | æˆåŠŸç‡ |")
            lines.append("|------|------|------|--------|")
            for strategy, stats in summary["by_strategy"].items():
                lines.append(
                    f"| {strategy} | {stats['success']} | "
                    f"{stats['total']} | {stats['success_rate']:.1f}% |"
                )
            lines.append("")
        
        # è¯¦ç»†é¡¹ç›®ç»“æœ
        lines.append("## ğŸ“‹ è¯¦ç»†æµ‹è¯•ç»“æœ")
        lines.append("")
        lines.append("| é¡¹ç›® | éš¾åº¦ | çŠ¶æ€ | æ—¶é—´(s) | è¿­ä»£ | é‡è¯• |")
        lines.append("|------|------|------|---------|------|------|")
        
        for result in results:
            status = "âœ…" if result.success else "âŒ"
            retry_text = ""
            if result.retry_info and result.retry_info.total_attempts > 1:
                retry_text = f"{result.retry_info.failed_attempts}æ¬¡"
            else:
                retry_text = "-"
            
            lines.append(
                f"| {result.project_name} | {result.project_difficulty} | "
                f"{status} | {result.deployment_time_seconds:.1f} | "
                f"{result.total_iterations} | {retry_text} |"
            )
        
        lines.append("")
        
        # å¤±è´¥é¡¹ç›®è¯¦æƒ…
        failed_results = [r for r in results if not r.success]
        if failed_results:
            lines.append("## âŒ å¤±è´¥é¡¹ç›®è¯¦æƒ…")
            lines.append("")
            for result in failed_results:
                lines.append(f"### {result.project_name}")
                lines.append("")
                lines.append(f"- **ä»“åº“**: {result.repo_url}")
                lines.append(f"- **éš¾åº¦**: {result.project_difficulty}")
                lines.append(f"- **é”™è¯¯**: {result.error or 'æœªçŸ¥é”™è¯¯'}")
                if result.retry_info:
                    lines.append(f"- **å°è¯•æ¬¡æ•°**: {result.retry_info.total_attempts}")
                    if result.retry_info.retry_reasons:
                        lines.append(f"- **é‡è¯•åŸå› **:")
                        for reason in result.retry_info.retry_reasons:
                            lines.append(f"  - {reason}")
                lines.append("")
        
        # å†™å…¥æ–‡ä»¶
        with open(report_file, "w", encoding="utf-8") as f:
            f.write("\n".join(lines))
        
        return report_file
    
    def print_enhanced_summary(
        self,
        summary: Dict[str, Any],
        system_info: Optional[SystemInfo] = None
    ):
        """
        æ‰“å°å¢å¼ºçš„æ§åˆ¶å°æ‘˜è¦
        
        Args:
            summary: èšåˆæ‘˜è¦
            system_info: ç³»ç»Ÿä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
        """
        print("\n" + "="*60)
        print("ğŸ“Š æµ‹è¯•æ‘˜è¦")
        print("="*60)
        
        if system_info:
            print(f"\nğŸ–¥ï¸  ç³»ç»Ÿç¯å¢ƒ:")
            print(f"   {system_info.os_name} {system_info.os_version}")
            print(f"   Python {system_info.python_version}")
            print(f"   {system_info.cpu_count} CPUæ ¸å¿ƒ, {system_info.memory_total_gb} GBå†…å­˜")
        
        print(f"\nğŸ“ˆ æµ‹è¯•ç»“æœ:")
        print(f"   æ€»é¡¹ç›®æ•°: {summary['total_projects']}")
        print(f"   æˆåŠŸ: {summary['successful']} âœ…")
        print(f"   å¤±è´¥: {summary['failed']} âŒ")
        print(f"   æˆåŠŸç‡: {summary['success_rate']:.1f}%")
        
        if summary.get('total_retries', 0) > 0:
            print(f"   æ€»é‡è¯•æ¬¡æ•°: {summary['total_retries']}")
        
        if summary.get('avg_metrics'):
            avg = summary['avg_metrics']
            print(f"\nâš¡ æ€§èƒ½ç»Ÿè®¡ï¼ˆä»…æˆåŠŸé¡¹ç›®ï¼‰:")
            print(f"   å¹³å‡éƒ¨ç½²æ—¶é—´: {avg['deployment_time_seconds']:.1f} ç§’")
            print(f"   å¹³å‡è¿­ä»£æ¬¡æ•°: {avg['iterations']:.1f}")
            print(f"   å¹³å‡LLMè°ƒç”¨: {avg['llm_calls']:.1f}")
        
        if summary.get('by_difficulty'):
            print(f"\nğŸ“Š æŒ‰éš¾åº¦åˆ†ç±»:")
            for diff in ["easy", "medium", "hard"]:
                if diff in summary["by_difficulty"]:
                    stats = summary["by_difficulty"][diff]
                    print(
                        f"   {diff.capitalize()}: {stats['success']}/{stats['total']} "
                        f"({stats['success_rate']:.1f}%)"
                    )
        
        print("="*60 + "\n")
    
    def _build_report_metadata(
        self,
        test_start_time: datetime,
        test_end_time: datetime,
        parallel_workers: int
    ) -> Dict[str, Any]:
        """æ„å»ºæŠ¥å‘Šå…ƒæ•°æ®"""
        duration_minutes = (test_end_time - test_start_time).total_seconds() / 60
        
        return {
            "generated_at": datetime.now().isoformat(),
            "test_start_time": test_start_time.isoformat(),
            "test_end_time": test_end_time.isoformat(),
            "test_duration_minutes": round(duration_minutes, 2),
            "parallel_workers": parallel_workers
        }
    
    def _build_environment_section(
        self,
        results: List[EnhancedProjectMetrics]
    ) -> Dict[str, Any]:
        """æ„å»ºç¯å¢ƒä¿¡æ¯éƒ¨åˆ†"""
        if not results:
            return {}
        
        first_result = results[0]
        env_section = {}
        
        if first_result.system_info:
            env_section["system"] = first_result.system_info.to_dict()
        
        if first_result.llm_config:
            env_section["llm_config"] = first_result.llm_config.to_dict()
        
        return env_section
    
    def _aggregate_enhanced_metrics(
        self,
        results: List[EnhancedProjectMetrics]
    ) -> Dict[str, Any]:
        """èšåˆå¢å¼ºæŒ‡æ ‡"""
        if not results:
            return {
                "total_projects": 0,
                "successful": 0,
                "failed": 0,
                "success_rate": 0.0,
                "total_retries": 0
            }
        
        total = len(results)
        successful = sum(1 for r in results if r.success)
        failed = total - successful
        success_rate = (successful / total * 100) if total > 0 else 0.0
        
        # ç»Ÿè®¡é‡è¯•æ¬¡æ•°
        total_retries = sum(
            r.retry_info.failed_attempts 
            for r in results 
            if r.retry_info
        )
        
        # æŒ‰éš¾åº¦åˆ†ç±»
        by_difficulty = self._calculate_multi_dimension_stats(
            results, 
            lambda r: r.project_difficulty
        )
        
        # æŒ‰ç­–ç•¥åˆ†ç±»
        by_strategy = self._calculate_multi_dimension_stats(
            results,
            lambda r: r.strategy_used
        )
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡ï¼ˆåªç»Ÿè®¡æˆåŠŸçš„é¡¹ç›®ï¼‰
        successful_results = [r for r in results if r.success]
        avg_metrics = {}
        
        if successful_results:
            avg_metrics = {
                "deployment_time_seconds": round(
                    statistics.mean([r.deployment_time_seconds for r in successful_results]), 1
                ),
                "iterations": round(
                    statistics.mean([r.total_iterations for r in successful_results]), 1
                ),
                "commands": round(
                    statistics.mean([r.total_commands for r in successful_results]), 1
                ),
                "llm_calls": round(
                    statistics.mean([r.llm_call_count for r in successful_results]), 1
                ),
            }
        
        return {
            "total_projects": total,
            "successful": successful,
            "failed": failed,
            "success_rate": round(success_rate, 1),
            "total_retries": total_retries,
            "by_difficulty": by_difficulty,
            "by_strategy": by_strategy,
            "avg_metrics": avg_metrics
        }
    
    def _calculate_multi_dimension_stats(
        self,
        results: List[EnhancedProjectMetrics],
        key_func
    ) -> Dict[str, Dict[str, Any]]:
        """
        æŒ‰æŒ‡å®šç»´åº¦è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        
        Args:
            results: ç»“æœåˆ—è¡¨
            key_func: æå–ç»´åº¦é”®çš„å‡½æ•°
            
        Returns:
            æŒ‰ç»´åº¦åˆ†ç»„çš„ç»Ÿè®¡ä¿¡æ¯
        """
        stats = {}
        
        for result in results:
            key = key_func(result)
            if key not in stats:
                stats[key] = {"total": 0, "success": 0}
            
            stats[key]["total"] += 1
            if result.success:
                stats[key]["success"] += 1
        
        # è®¡ç®—æˆåŠŸç‡
        for key in stats:
            total = stats[key]["total"]
            success = stats[key]["success"]
            stats[key]["success_rate"] = round(
                (success / total * 100) if total > 0 else 0.0, 
                1
            )
        
        return stats
